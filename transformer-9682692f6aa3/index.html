

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#0D1716">
  <meta name="author" content="Cybersh1t">
  <meta name="keywords" content="">
  
    <meta name="description" content="写在前面在Transformer出现之前，NLP的通解经历了几种架构的变迁：  第一阶段：混沌、机械的统计机器学习为代表的方法。  第二阶段：以word2vec为代表的方法。  第三阶段：以bert为代表的方法。   第一阶段中，NLP对于每一个序列中的元素表示没有一个统一的方法。据我了解的方法就有如下：  one-hot表示一个词 bag-of-words来表示一段文本 tf-idf中用频率的手">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer杂记">
<meta property="og:url" content="https://blog.cybercolyce.cn/transformer-9682692f6aa3/index.html">
<meta property="og:site_name" content="Laked22">
<meta property="og:description" content="写在前面在Transformer出现之前，NLP的通解经历了几种架构的变迁：  第一阶段：混沌、机械的统计机器学习为代表的方法。  第二阶段：以word2vec为代表的方法。  第三阶段：以bert为代表的方法。   第一阶段中，NLP对于每一个序列中的元素表示没有一个统一的方法。据我了解的方法就有如下：  one-hot表示一个词 bag-of-words来表示一段文本 tf-idf中用频率的手">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/687606f558cb8da5c8b1da38.png">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6875b0f558cb8da5c8ae5856.png">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6875b7ee58cb8da5c8ae830d.png">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6875cbd358cb8da5c8af624e.png">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6875cc5558cb8da5c8af688a.png">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6875cf2058cb8da5c8af72b3.png">
<meta property="article:published_time" content="2025-07-15T16:02:35.359Z">
<meta property="article:modified_time" content="2025-07-15T16:04:42.387Z">
<meta property="article:author" content="Cybersh1t">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="优化">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/687606f558cb8da5c8b1da38.png">
  
  
  
  <title>Transformer杂记 - Laked22</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.cybercolyce.cn","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Laked22</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/imgs/background.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">Transformer杂记</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-16 00:02" pubdate>
          2025年7月16日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Transformer杂记</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在Transformer出现之前，NLP的通解经历了几种架构的变迁：</p>
<ul>
<li><p>第一阶段：混沌、机械的统计机器学习为代表的方法。</p>
</li>
<li><p>第二阶段：以word2vec为代表的方法。</p>
</li>
<li><p>第三阶段：以bert为代表的方法。</p>
</li>
</ul>
<p>第一阶段中，NLP对于每一个序列中的元素表示没有一个统一的方法。据我了解的方法就有如下：</p>
<ul>
<li>one-hot表示一个词</li>
<li>bag-of-words来表示一段文本</li>
<li>tf-idf中用频率的手段来表征词语的重要性</li>
<li>text-rank中借鉴了page-rank的方法来表征词语的权重</li>
<li>基于SVD纯数学分解词文档矩阵的LSA</li>
<li>pLSA中用概率手段来表征文档形成过程</li>
<li>LDA中引入两个共轭分布从而完美引入先验</li>
</ul>
<blockquote>
<p>仅需要关注前三个即可，第四个开始其实业界研究、使用都比较少了，但作为一种思想可以学习。</p>
</blockquote>
<p>第二阶段是技术积累阶段。【TODO】</p>
<p>第三阶段则是现在大航海的阶段。现在的大家说起大模型第一个想到的就是OpenAI提出的GPT，它开启了大规模语料集预训练模型的时代。GPT其实是“Improving Language Understanding by Generative Pre-Training”这篇论文中的Generative Pre-Traning的缩写。这篇论文中使用的核心技术则是谷歌在2017年提出的Transformer架构，也就是后续我们需要学习的对象。</p>
<blockquote>
<p>插个题外话，谷歌和百度这两家在我看来都有一定的相似性。二者都以搜索业发家，并且都具有一定的技术前瞻性，但却没有很好的落地能力。百度早在2013年就开始布局自动驾驶，但今天却难以在自动驾驶的智驾系统中立足；文心一言大模型在历经技术更迭后却不能及时推广出去导致日活越来越低，加上各类负面新闻使得百度成为“起个大早赶个晚集”的代表；谷歌拥有多个行业的技术积累，但却没有深入挖掘落地场景，事实上谷歌的能力并不差：2018年战胜李世石的AlphaGo、深度强化学习的开山之作DQN均诞生于谷歌旗下。这不禁让人思考：技术是否真的是科技行业的一切？</p>
</blockquote>
<h2 id="Transformer的感性认知"><a href="#Transformer的感性认知" class="headerlink" title="Transformer的感性认知"></a>Transformer的感性认知</h2><p>Transformer模型提出之始是用于自然语言处理（Natural Language Processing，NLP）任务的，NLP的典型任务类型本质是<strong>语言序列处理</strong>。一个序列是若干语言元素（如单词、标点）的集合，NLP的多数任务是基于这种不定长的输入进行不定长的输出。实际上，语言模型的本质是对一段自然语言的文本进行预测概率的大小，即：<br>$$<br>P(s)&#x3D;P(w_1w_2w_3…w_t) &#x3D;P(w_1)P(w_2|w_1)P(w_3|w_1w_2)P(w_T|w_1…w_{T-1})<br>$$</p>
<p>另外一个值得关注的Transformer特征就是query 、 key &amp; value 的概念。query 、 key &amp; value 的概念其实来源于推荐系统。基本原理是：给定一个 query，计算query 与 key 的相关性，然后根据query 与 key 的相关性去找到最合适的 value。举个例子：在电影推荐中。query 是某个人对电影的喜好信息（比如兴趣点、年龄、性别等）、key 是电影的类型（喜剧、年代等）、value 就是待推荐的电影。在这个例子中，query, key 和 value 的每个属性虽然在不同的空间，其实他们是有一定的潜在关系的，也就是说通过某种变换，可以使得三者的属性在一个相近的空间中。</p>
<p>Transformer的结构图是这样的：</p>
<p><img src="https://pic1.imgdb.cn/item/687606f558cb8da5c8b1da38.png" srcset="/img/loading.gif" lazyload alt="Transformer 结构"></p>
<p>它由Encoder编码器和Decoder解码器两个部分组成，并在最后的输出层使用一个全连接层FC以及Softmax来输出对应的概率。</p>
<h2 id="计算流程"><a href="#计算流程" class="headerlink" title="计算流程"></a>计算流程</h2><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>特征嵌入（Embedding）是一个很有意思的特征变换过程。一般来说，他会用在特征尺度不统一的情况下的变换，或者数据的增维、降维。如果时间允许，建议阅读知乎的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/164502624">这篇文章</a>。在Transformer的论文中，embedding的尺寸为512维。</p>
<h3 id="自注意力-Self-Attetnion"><a href="#自注意力-Self-Attetnion" class="headerlink" title="自注意力 Self Attetnion"></a>自注意力 Self Attetnion</h3><p>自注意力计算如下：<br>$$<br>Z(Q,K,V) &#x3D; Softmax(\frac{QK^{T}}{\sqrt{d}})V<br>$$<br>在计算开始前，需要明白如下物理量的意义：</p>
<ul>
<li><p>$Q,K,V$ 均为矩阵，其行数为输入序列的长度，其列数为embedding的维度。</p>
</li>
<li><p>$d$ 为embedding结果 $X$到QKV的映射矩阵$W_i$的列数，在Transformer的论文中这个值为64。</p>
</li>
</ul>
<blockquote>
<p>严格来说，这里的64是拆分后的结果。映射矩阵$W_q,W_k,W_v$本身是一个$512\times 512$的矩阵，只不过被按列拆成了8份，每一份的维度列数为512&#x2F;8&#x3D;8.</p>
</blockquote>
<p>为了方便说明，我们称$Q$和$K$转置的积为score，最终的注意力计算结果为Z。</p>
<p>以简单的句子<code>&quot;Going Home&quot;</code>为例，演示其输入后的计算流程。首先先是得到QKV矩阵的流程：<br><img src="https://pic1.imgdb.cn/item/6875b0f558cb8da5c8ae5856.png" srcset="/img/loading.gif" lazyload alt="前置计算"></p>
<p>随后则是根据自注意力计算公式进行计算：</p>
<p><img src="https://pic1.imgdb.cn/item/6875b7ee58cb8da5c8ae830d.png" srcset="/img/loading.gif" lazyload alt="自注意力计算"></p>
<h3 id="多头自注意力MultiHead-Attention-MHA"><a href="#多头自注意力MultiHead-Attention-MHA" class="headerlink" title="多头自注意力MultiHead Attention(MHA)"></a>多头自注意力MultiHead Attention(MHA)</h3><p>为了进一步细化自注意力机制层，增加了“多头注意力机制”的概念。 对于多头自注意力机制，则不止有一组Q&#x2F;K&#x2F;V权重矩阵，而是有多组（论文中使用8组），所以每个编码器&#x2F;解码器使用8个“头”（可以理解为8个互不干扰自的注意力机制运算），每一组的Q&#x2F;K&#x2F;V都不相同。然后，得到8个不同的权重矩阵Z，每个权重矩阵被用来将输入向量投射到不同的表示子空间。</p>
<p>如果硬要公式来表示的话，MHA的计算流程如下：</p>
<p>$$<br>\begin{aligned}</p>
<p>&amp;Q_i &#x3D; QW_i^Q,K_i &#x3D; KW_i^K,V_i &#x3D; VW_i^V,\quad where \quad i\in{1,2,\cdots,n}\<br>&amp;head_i &#x3D; Attention(Q_i,K_i,V_i) ,\quad where \quad i\in{1,2,\cdots,n}\<br>&amp;MHA(Q,K,V) &#x3D; Concat(head_1,head_2,\cdots,head_n)\cdot W^o<br>\end{aligned}<br>$$</p>
<p>在Transformer中使用的是MultiHead Attention，其实这玩意和Self Attention区别并不是很大。先明确以下几点：</p>
<ol>
<li>MultiHead的head不管有几个，每个head的参数量都是一样的。并不是head多，参数就多。</li>
<li>当多头的head数量为1时，并不等价于Self Attetnion，MultiHead Attention和Self Attention是不一样的东西。</li>
<li>多头注意力使用的也是自注意力计算的公式。</li>
<li>MultiHead除了 $W_q$, $W_k$, $W_v$ 三个矩阵外，还要多额外定义一个$W_o$。<br>而MultiHead Attention在带入公式前做了一件事情，就是拆，它按照“词向量维度（即将512维度进行拆分）”这个方向，将Q,K,V拆成了多个头，在这里以拆分成四个头为例进行演示，如图所示：<br><img src="https://pic1.imgdb.cn/item/6875cbd358cb8da5c8af624e.png" srcset="/img/loading.gif" lazyload alt="QKV按列拆分"></li>
</ol>
<p>随后进行同样的自注意力计算流程得到四个头对应的Attention值：<br><img src="https://pic1.imgdb.cn/item/6875cc5558cb8da5c8af688a.png" srcset="/img/loading.gif" lazyload></p>
<p>这样得到的拆分Attention结果如果直接Concat效果是肯定不太好的，所以最后需要再采用一个额外的Wo矩阵，对Attention再进行一次线性变换。$W_o$其实也可以看作是一次Embedding，用于将不同维度中的信息进行融合。</p>
<p>注意$W_o$矩阵不需要八个，对于第5节中的案例，两组词向量2x512对应的原始维度为512x512的WQ、WK和WV被拆分为8个512x64的矩阵，故计算后得到8个2x64的Z矩阵，形式类似下图：</p>
<p><img src="https://pic1.imgdb.cn/item/6875cf2058cb8da5c8af72b3.png" srcset="/img/loading.gif" lazyload></p>
<p>只需要把这些注意力结果的$Z$矩阵拼接起来然后用一个额外的权重矩阵Wo与之相乘即可。</p>
<h3 id="残差结构"><a href="#残差结构" class="headerlink" title="残差结构"></a>残差结构</h3><p>不做详细说明</p>
<h3 id="Decoder解码器"><a href="#Decoder解码器" class="headerlink" title="Decoder解码器"></a>Decoder解码器</h3><p> 论文中的结构图采用的是Outputs进行表示，其实是整个Transformer上一时刻的输出作为Decoder的输入。具体可以分类为训练时的输入和预测时的输入。</p>
<p> 训练时：就是对已经准备好对应的target数据（类似CV groundtruth标定）。例如翻译任务，Encoder输入“Tom chase Jerry”，Decoder输入对应的中文翻译标定“汤姆追逐杰瑞”。</p>
<p> 预测时：预测时Decoder的输入其实按词向量的个数来做循环输入，Decoder首次输入为起始符，然后每次的输入是上一时刻Transformer的输出。例如，Decoder首次输入“”，Decoder输出为本时刻预测结果“汤姆”；然后Decoder输入“汤姆”，Decoder输出预测结果“汤姆追逐”；之后Decoder输入“汤姆追逐”，则输出预测结果“汤姆追逐杰瑞”；最后Decoder输入“汤姆追逐杰瑞”，预测输出“汤姆追逐杰瑞”，此时输入输出一致，则结果整个流程。</p>
<p> 可以看出，预测阶段的Decoder其实是一个循环推理的过程，为了词的前后关联性，从而根据分词方式来确定时间序列的长度t（Decoder执行的次数）。而前述的Encoder的输出给到Decoder作为输入，其实是Encoder给了Decoder的Multi-Head Attention提供可Keys和Values矩阵。故6个叠加的Decoder中每个注意力均使用同一个Keys和Values矩阵。整个流程可以使用下面的一个流程范例来帮助理解。</p>
<p>  假设我们是要用Transformer翻译“Machine learning is fun”这句话。首先，我们会将“Machine learning is fun” 送给Encoder，输出一个名叫Memory的Tensor，之后我们会将该Memory作为Decoder的一个输入，使用Decoder预测。Decoder并不是一下子就能把“机器学习真好玩”说出来，而是一个词一个词说（或一个字一个字，这取决于你的分词方式），首次输入起始值<code>&lt;bos&gt;</code>(begin of sentence的缩写)，Decoder根据Memory推理输出“机”字， 紧接着，我们会再次调用Decoder，这次是传入<code>&quot;&lt;bos&gt; 机&quot;</code>，然后推理得到“器”字；依次类推，直到最后输出<code>&lt;eos&gt;</code>结束，当Transformer输出<code>&lt;eos&gt;</code>时，预测就结束了。</p>
<h3 id="掩码多头注意力-Masked-MultiHead-attention"><a href="#掩码多头注意力-Masked-MultiHead-attention" class="headerlink" title="掩码多头注意力 Masked MultiHead-attention"></a>掩码多头注意力 Masked MultiHead-attention</h3><p>由于解码器采用自回归auto-regressive，即 在过去时刻的输出作为当前时刻的输入，也就是说在预测时无法看到之后的输入输出，但是在注意力机制当中，可以看到完整的输入(每一个词都要和其他词做点积，计算相关性)，为了避免这种情况的发生，在<strong>解码器训练时</strong>，在<strong>预测t时刻的输出时</strong>，不应该能看到t时刻以后的输入。做法是：采用带掩码的Masked注意力机制,从而保证在t时刻无法看到t时刻以后的输入，保证训练和预测时的行为一致性。</p>
<p>所以，Masked-MultiHead-attention的其它部分计算流程实际上与Encoder中的计算过程一致，区别只是在计算出scores矩阵时对其沿对角线上部分进行mask掩码。其主要在训练阶段屏蔽t时刻之后的输入生效，而在预测阶段其实并没有真实作用。如第10节所举之例，预测“机器学习真好玩”的过程。对于Decoder来说是一个字一个字预测的，所以假设我们Decoder的输入是“机器学习”时，“习”字只能看到前面的“机器学”三个字，所以此时对于“习”字只有“机器学习”四个字的注意力信息。但是，训练阶段传的是“机器学习真好玩”，还是不能让“习”字看到后面“真好玩”三个字，所以要使用mask将其盖住，这又是为什么呢？原因是：训练阶段是为了让每个词只看到序列（因为训练时解码器端输入整个序列）中前面的词，而测试阶段是为了防止编码发生变化。</p>
<h1 id="mask掩码的方式，原理很简单，self-attention的输出是经过softmax的scores向量，故对应scores屏蔽即可。Decoder是逐个接收输入后进行输出的，那么在t时刻，共有-v-t-个输入向量。那么score矩阵-A-alpha-、输入词向量-V-v-i-0-leq-i-leq-t-以及输出-O-满足如下关系：-begin-bmatrix-o-1-o-2-vdots-o-t-end-bmatrix"><a href="#mask掩码的方式，原理很简单，self-attention的输出是经过softmax的scores向量，故对应scores屏蔽即可。Decoder是逐个接收输入后进行输出的，那么在t时刻，共有-v-t-个输入向量。那么score矩阵-A-alpha-、输入词向量-V-v-i-0-leq-i-leq-t-以及输出-O-满足如下关系：-begin-bmatrix-o-1-o-2-vdots-o-t-end-bmatrix" class="headerlink" title="mask掩码的方式，原理很简单，self-attention的输出是经过softmax的scores向量，故对应scores屏蔽即可。Decoder是逐个接收输入后进行输出的，那么在t时刻，共有$v_t$个输入向量。那么score矩阵$A(\alpha)$、输入词向量$V&#x3D;{v_i|0\leq i\leq t}$以及输出$O$满足如下关系：$$\begin{bmatrix}o_1\o_2\\vdots\o_t\end{bmatrix}"></a>mask掩码的方式，原理很简单，self-attention的输出是经过softmax的scores向量，故对应scores屏蔽即可。Decoder是逐个接收输入后进行输出的，那么在t时刻，共有$v_t$个输入向量。那么score矩阵$A(\alpha)$、输入词向量$V&#x3D;{v_i|0\leq i\leq t}$以及输出$O$满足如下关系：<br>$$<br>\begin{bmatrix}<br>o_1\o_2\\vdots\o_t<br>\end{bmatrix}</h1><p>\begin{bmatrix}<br>\alpha_{1,1} &amp; 0 &amp; \cdots &amp; 0\<br>\alpha_{2,1} &amp; \alpha_{2,2} &amp; \cdots &amp; 0\<br>\vdots &amp;\vdots &amp; \quad &amp; \vdots\</p>
<p>\alpha_{t,1} &amp; \alpha_{t,2} &amp; \cdots &amp; \alpha_{t,t}<br>\end{bmatrix}<br>\begin{bmatrix}<br>v_1\v_2\\vdots\v_t<br>\end{bmatrix}<br>$$</p>
<p>也就是说，对矩阵中对角线以上的元素使用0进行遮罩即可，在Transformer的代码中，scores矩阵中的mask掩码并非置0，而是使用了-1e9（负无穷）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>)<br>p_attn = scores.softmax(dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>这是因为，mask为0的话，注意力矩阵经过softmax中的指数运算后，会在遮罩部分有残留的值。因此应该mask为一个远远小于0的数，例如-1e9。</p>
<blockquote>
<p>回忆Softmax函数：$Softmax(X) &#x3D; \frac{e^{x_i}}{\sum_i e^{x_i}}$。对于$x_i&#x3D;0$而言，指数运算后的值并非为0。</p>
</blockquote>
<h3 id="最终输出"><a href="#最终输出" class="headerlink" title="最终输出"></a>最终输出</h3><p>Liner和Softmax层将解码器最终输出的实数向量变成一个word，线性变换层是全连接神经网络，将解码器产生的向量投影到一个比它大得多的、被称作对数几率（logits）的向量里。假设模型从训练集中学习一万个不同的word，则对数几率向量为一万个单元格长度的向量，每个单元格对应某一个单词的分数。softmax层将分数变成概率，概率最高的单元格对应的单词被作为该时间的输出。</p>
<p>仍按前例，如输入一个2x512的词向量矩阵，前述Decoder重复执行6次后得到一个2x64的矩阵，然后进过FC（Linear）线性变换后得到一个1x2的向量，进行最终的softmax评分得到最高分数即为target中某个词的概率。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/xunan003/article/details/130080111">一文理解Transformer整套流程</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
        <a href="/tags/%E4%BC%98%E5%8C%96/" class="print-no-link">#优化</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/KVCache-beeb481dee12/" title="KVCache初探">
                        <span class="hidden-mobile">KVCache初探</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
      <p>Cybersh1t@2025</p>
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
